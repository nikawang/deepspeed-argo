#0 building with "default" instance using docker driver

#1 [internal] load build definition from Dockerfile
#1 transferring dockerfile: 283B done
#1 DONE 0.1s

#2 [internal] load metadata for nikawang.azurecr.io/deepspeed-bert:v1
#2 ...

#3 [auth] deepspeed-bert:pull token for nikawang.azurecr.io
#3 DONE 0.0s

#2 [internal] load metadata for nikawang.azurecr.io/deepspeed-bert:v1
#2 DONE 1.7s

#4 [internal] load .dockerignore
#4 transferring context: 2B done
#4 DONE 0.0s

#5 [1/7] FROM nikawang.azurecr.io/deepspeed-bert:v1@sha256:e1bf2b21d60d4d1a56d9efec1fc8dcb337d426ccb279bc65547813056cfc5aa2
#5 DONE 0.0s

#6 [internal] load build context
#6 transferring context: 99B done
#6 DONE 0.0s

#7 [3/7] COPY ./run-init.sh /app/run-init.sh
#7 CACHED

#8 [4/7] COPY ./requirements.txt /app/requirements.txt
#8 CACHED

#9 [5/7] RUN pip install -r /app/requirements.txt
#9 CACHED

#10 [6/7] RUN bash /app/run-init.sh
#10 CACHED

#11 [2/7] WORKDIR /app
#11 CACHED

#12 [7/7] COPY ./run-175b.sh  /app/run-175.sh
#12 CACHED

#13 exporting to image
#13 exporting layers done
#13 writing image sha256:72b2138c418af2dc857a587117f77f8687eb280c0964264ab9a179e9f9405510 0.0s done
#13 naming to nikawang.azurecr.io/deepspeed-megatron-gpt-boostrap 0.0s done
#13 DONE 0.0s
#0 building with "default" instance using docker driver

#1 [internal] load build definition from Dockerfile
#1 transferring dockerfile: 283B done
#1 DONE 0.1s

#2 [internal] load metadata for nikawang.azurecr.io/deepspeed-bert:v1
#2 ...

#3 [auth] deepspeed-bert:pull token for nikawang.azurecr.io
#3 DONE 0.0s

#2 [internal] load metadata for nikawang.azurecr.io/deepspeed-bert:v1
#2 DONE 1.6s

#4 [internal] load .dockerignore
#4 transferring context: 2B done
#4 DONE 0.1s

#5 [1/7] FROM nikawang.azurecr.io/deepspeed-bert:v1@sha256:e1bf2b21d60d4d1a56d9efec1fc8dcb337d426ccb279bc65547813056cfc5aa2
#5 DONE 0.0s

#6 [internal] load build context
#6 transferring context: 920B done
#6 DONE 0.0s

#7 [2/7] WORKDIR /app
#7 CACHED

#8 [3/7] COPY ./run-init.sh /app/run-init.sh
#8 DONE 0.7s

#9 [4/7] COPY ./requirements.txt /app/requirements.txt
#9 DONE 0.5s

#10 [5/7] RUN pip install -r /app/requirements.txt
#10 0.861 Collecting nltk==3.8.1 (from -r /app/requirements.txt (line 1))
#10 0.913   Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)
#10 1.035      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 12.6 MB/s eta 0:00:00
#10 1.124 Collecting torchvision==0.16.2 (from -r /app/requirements.txt (line 2))
#10 1.129   Downloading torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)
#10 1.164 Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->-r /app/requirements.txt (line 1)) (8.1.7)
#10 1.215 Collecting joblib (from nltk==3.8.1->-r /app/requirements.txt (line 1))
#10 1.219   Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)
#10 1.244 Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->-r /app/requirements.txt (line 1)) (2023.12.25)
#10 1.245 Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->-r /app/requirements.txt (line 1)) (4.62.3)
#10 1.248 Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.2->-r /app/requirements.txt (line 2)) (1.26.3)
#10 1.249 Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.2->-r /app/requirements.txt (line 2)) (2.31.0)
#10 1.250 Requirement already satisfied: torch==2.1.2 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.2->-r /app/requirements.txt (line 2)) (2.1.2)
#10 1.446 Collecting pillow!=8.3.*,>=5.3.0 (from torchvision==0.16.2->-r /app/requirements.txt (line 2))
#10 1.451   Downloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.7 kB)
#10 1.502 Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision==0.16.2->-r /app/requirements.txt (line 2)) (3.13.1)
#10 1.503 Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision==0.16.2->-r /app/requirements.txt (line 2)) (4.9.0)
#10 1.503 Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision==0.16.2->-r /app/requirements.txt (line 2)) (1.12)
#10 1.504 Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision==0.16.2->-r /app/requirements.txt (line 2)) (3.2.1)
#10 1.504 Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision==0.16.2->-r /app/requirements.txt (line 2)) (3.1.3)
#10 1.505 Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision==0.16.2->-r /app/requirements.txt (line 2)) (2023.10.0)
#10 1.506 Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision==0.16.2->-r /app/requirements.txt (line 2)) (12.1.105)
#10 1.507 Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision==0.16.2->-r /app/requirements.txt (line 2)) (12.1.105)
#10 1.509 Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision==0.16.2->-r /app/requirements.txt (line 2)) (12.1.105)
#10 1.510 Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision==0.16.2->-r /app/requirements.txt (line 2)) (8.9.2.26)
#10 1.511 Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision==0.16.2->-r /app/requirements.txt (line 2)) (12.1.3.1)
#10 1.512 Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision==0.16.2->-r /app/requirements.txt (line 2)) (11.0.2.54)
#10 1.514 Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision==0.16.2->-r /app/requirements.txt (line 2)) (10.3.2.106)
#10 1.515 Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision==0.16.2->-r /app/requirements.txt (line 2)) (11.4.5.107)
#10 1.516 Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision==0.16.2->-r /app/requirements.txt (line 2)) (12.1.0.106)
#10 1.518 Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision==0.16.2->-r /app/requirements.txt (line 2)) (2.18.1)
#10 1.519 Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision==0.16.2->-r /app/requirements.txt (line 2)) (12.1.105)
#10 1.520 Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision==0.16.2->-r /app/requirements.txt (line 2)) (2.1.0)
#10 1.529 Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2->torchvision==0.16.2->-r /app/requirements.txt (line 2)) (12.3.101)
#10 1.569 Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.2->-r /app/requirements.txt (line 2)) (3.3.2)
#10 1.569 Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.2->-r /app/requirements.txt (line 2)) (3.6)
#10 1.570 Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.2->-r /app/requirements.txt (line 2)) (2.1.0)
#10 1.571 Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.2->-r /app/requirements.txt (line 2)) (2023.11.17)
#10 1.649 Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.2->torchvision==0.16.2->-r /app/requirements.txt (line 2)) (2.1.3)
#10 1.668 Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.2->torchvision==0.16.2->-r /app/requirements.txt (line 2)) (1.3.0)
#10 1.690 Downloading torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl (6.8 MB)
#10 1.909    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.8/6.8 MB 31.3 MB/s eta 0:00:00
#10 1.918 Downloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)
#10 2.055    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.5/4.5 MB 33.2 MB/s eta 0:00:00
#10 2.060 Downloading joblib-1.3.2-py3-none-any.whl (302 kB)
#10 2.129    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 302.2/302.2 kB 4.4 MB/s eta 0:00:00
#10 3.012 Installing collected packages: pillow, joblib, nltk, torchvision
#10 4.449 Successfully installed joblib-1.3.2 nltk-3.8.1 pillow-10.2.0 torchvision-0.16.2
#10 4.449 WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
#10 DONE 6.1s

#11 [6/7] RUN bash /app/run-init.sh
#11 0.526 Reading package lists...
#11 1.137 Building dependency tree...
#11 1.260 Reading state information...
#11 1.358 The following additional packages will be installed:
#11 1.359   libgpm2 libsodium23 vim-common vim-runtime xxd
#11 1.359 Suggested packages:
#11 1.359   gpm ctags vim-doc vim-scripts
#11 1.388 The following NEW packages will be installed:
#11 1.389   libgpm2 libsodium23 vim vim-common vim-runtime wget xxd
#11 2.142 0 upgraded, 7 newly installed, 0 to remove and 49 not upgraded.
#11 2.142 Need to get 9253 kB of archives.
#11 2.142 After this operation, 39.8 MB of additional disk space will be used.
#11 2.142 Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xxd amd64 2:8.2.3995-1ubuntu2.15 [55.2 kB]
#11 2.988 Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 vim-common all 2:8.2.3995-1ubuntu2.15 [81.5 kB]
#11 3.280 Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 wget amd64 1.21.2-2ubuntu1 [367 kB]
#11 3.847 Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgpm2 amd64 1.20.7-10build1 [15.3 kB]
#11 3.860 Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsodium23 amd64 1.0.18-1build2 [164 kB]
#11 3.962 Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 vim-runtime all 2:8.2.3995-1ubuntu2.15 [6835 kB]
#11 4.937 Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 vim amd64 2:8.2.3995-1ubuntu2.15 [1735 kB]
#11 5.345 debconf: delaying package configuration, since apt-utils is not installed
#11 5.384 Fetched 9253 kB in 4s (2462 kB/s)
#11 5.441 Selecting previously unselected package xxd.
#11 5.441 (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 21109 files and directories currently installed.)
#11 5.451 Preparing to unpack .../0-xxd_2%3a8.2.3995-1ubuntu2.15_amd64.deb ...
#11 5.471 Unpacking xxd (2:8.2.3995-1ubuntu2.15) ...
#11 5.653 Selecting previously unselected package vim-common.
#11 5.655 Preparing to unpack .../1-vim-common_2%3a8.2.3995-1ubuntu2.15_all.deb ...
#11 5.676 Unpacking vim-common (2:8.2.3995-1ubuntu2.15) ...
#11 5.802 Selecting previously unselected package wget.
#11 5.804 Preparing to unpack .../2-wget_1.21.2-2ubuntu1_amd64.deb ...
#11 5.826 Unpacking wget (1.21.2-2ubuntu1) ...
#11 5.995 Selecting previously unselected package libgpm2:amd64.
#11 5.997 Preparing to unpack .../3-libgpm2_1.20.7-10build1_amd64.deb ...
#11 6.033 Unpacking libgpm2:amd64 (1.20.7-10build1) ...
#11 6.193 Selecting previously unselected package libsodium23:amd64.
#11 6.195 Preparing to unpack .../4-libsodium23_1.0.18-1build2_amd64.deb ...
#11 6.212 Unpacking libsodium23:amd64 (1.0.18-1build2) ...
#11 6.362 Selecting previously unselected package vim-runtime.
#11 6.364 Preparing to unpack .../5-vim-runtime_2%3a8.2.3995-1ubuntu2.15_all.deb ...
#11 6.387 Adding 'diversion of /usr/share/vim/vim82/doc/help.txt to /usr/share/vim/vim82/doc/help.txt.vim-tiny by vim-runtime'
#11 6.411 Adding 'diversion of /usr/share/vim/vim82/doc/tags to /usr/share/vim/vim82/doc/tags.vim-tiny by vim-runtime'
#11 6.422 Unpacking vim-runtime (2:8.2.3995-1ubuntu2.15) ...
#11 6.888 Selecting previously unselected package vim.
#11 6.890 Preparing to unpack .../6-vim_2%3a8.2.3995-1ubuntu2.15_amd64.deb ...
#11 6.925 Unpacking vim (2:8.2.3995-1ubuntu2.15) ...
#11 7.110 Setting up libsodium23:amd64 (1.0.18-1build2) ...
#11 7.189 Setting up libgpm2:amd64 (1.20.7-10build1) ...
#11 7.270 Setting up wget (1.21.2-2ubuntu1) ...
#11 7.355 Setting up xxd (2:8.2.3995-1ubuntu2.15) ...
#11 7.464 Setting up vim-common (2:8.2.3995-1ubuntu2.15) ...
#11 7.568 Setting up vim-runtime (2:8.2.3995-1ubuntu2.15) ...
#11 7.692 Setting up vim (2:8.2.3995-1ubuntu2.15) ...
#11 7.743 update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/vim (vim) in auto mode
#11 7.754 update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/vimdiff (vimdiff) in auto mode
#11 7.764 update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/rvim (rvim) in auto mode
#11 7.775 update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/rview (rview) in auto mode
#11 7.790 update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/vi (vi) in auto mode
#11 7.790 update-alternatives: warning: skip creation of /usr/share/man/da/man1/vi.1.gz because associated file /usr/share/man/da/man1/vim.1.gz (of link group vi) doesn't exist
#11 7.790 update-alternatives: warning: skip creation of /usr/share/man/de/man1/vi.1.gz because associated file /usr/share/man/de/man1/vim.1.gz (of link group vi) doesn't exist
#11 7.790 update-alternatives: warning: skip creation of /usr/share/man/fr/man1/vi.1.gz because associated file /usr/share/man/fr/man1/vim.1.gz (of link group vi) doesn't exist
#11 7.790 update-alternatives: warning: skip creation of /usr/share/man/it/man1/vi.1.gz because associated file /usr/share/man/it/man1/vim.1.gz (of link group vi) doesn't exist
#11 7.790 update-alternatives: warning: skip creation of /usr/share/man/ja/man1/vi.1.gz because associated file /usr/share/man/ja/man1/vim.1.gz (of link group vi) doesn't exist
#11 7.790 update-alternatives: warning: skip creation of /usr/share/man/pl/man1/vi.1.gz because associated file /usr/share/man/pl/man1/vim.1.gz (of link group vi) doesn't exist
#11 7.790 update-alternatives: warning: skip creation of /usr/share/man/ru/man1/vi.1.gz because associated file /usr/share/man/ru/man1/vim.1.gz (of link group vi) doesn't exist
#11 7.790 update-alternatives: warning: skip creation of /usr/share/man/man1/vi.1.gz because associated file /usr/share/man/man1/vim.1.gz (of link group vi) doesn't exist
#11 7.803 update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/view (view) in auto mode
#11 7.803 update-alternatives: warning: skip creation of /usr/share/man/da/man1/view.1.gz because associated file /usr/share/man/da/man1/vim.1.gz (of link group view) doesn't exist
#11 7.803 update-alternatives: warning: skip creation of /usr/share/man/de/man1/view.1.gz because associated file /usr/share/man/de/man1/vim.1.gz (of link group view) doesn't exist
#11 7.804 update-alternatives: warning: skip creation of /usr/share/man/fr/man1/view.1.gz because associated file /usr/share/man/fr/man1/vim.1.gz (of link group view) doesn't exist
#11 7.804 update-alternatives: warning: skip creation of /usr/share/man/it/man1/view.1.gz because associated file /usr/share/man/it/man1/vim.1.gz (of link group view) doesn't exist
#11 7.804 update-alternatives: warning: skip creation of /usr/share/man/ja/man1/view.1.gz because associated file /usr/share/man/ja/man1/vim.1.gz (of link group view) doesn't exist
#11 7.804 update-alternatives: warning: skip creation of /usr/share/man/pl/man1/view.1.gz because associated file /usr/share/man/pl/man1/vim.1.gz (of link group view) doesn't exist
#11 7.804 update-alternatives: warning: skip creation of /usr/share/man/ru/man1/view.1.gz because associated file /usr/share/man/ru/man1/vim.1.gz (of link group view) doesn't exist
#11 7.804 update-alternatives: warning: skip creation of /usr/share/man/man1/view.1.gz because associated file /usr/share/man/man1/vim.1.gz (of link group view) doesn't exist
#11 7.815 update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/ex (ex) in auto mode
#11 7.815 update-alternatives: warning: skip creation of /usr/share/man/da/man1/ex.1.gz because associated file /usr/share/man/da/man1/vim.1.gz (of link group ex) doesn't exist
#11 7.815 update-alternatives: warning: skip creation of /usr/share/man/de/man1/ex.1.gz because associated file /usr/share/man/de/man1/vim.1.gz (of link group ex) doesn't exist
#11 7.815 update-alternatives: warning: skip creation of /usr/share/man/fr/man1/ex.1.gz because associated file /usr/share/man/fr/man1/vim.1.gz (of link group ex) doesn't exist
#11 7.815 update-alternatives: warning: skip creation of /usr/share/man/it/man1/ex.1.gz because associated file /usr/share/man/it/man1/vim.1.gz (of link group ex) doesn't exist
#11 7.815 update-alternatives: warning: skip creation of /usr/share/man/ja/man1/ex.1.gz because associated file /usr/share/man/ja/man1/vim.1.gz (of link group ex) doesn't exist
#11 7.815 update-alternatives: warning: skip creation of /usr/share/man/pl/man1/ex.1.gz because associated file /usr/share/man/pl/man1/vim.1.gz (of link group ex) doesn't exist
#11 7.815 update-alternatives: warning: skip creation of /usr/share/man/ru/man1/ex.1.gz because associated file /usr/share/man/ru/man1/vim.1.gz (of link group ex) doesn't exist
#11 7.815 update-alternatives: warning: skip creation of /usr/share/man/man1/ex.1.gz because associated file /usr/share/man/man1/vim.1.gz (of link group ex) doesn't exist
#11 7.826 update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/editor (editor) in auto mode
#11 7.827 update-alternatives: warning: skip creation of /usr/share/man/da/man1/editor.1.gz because associated file /usr/share/man/da/man1/vim.1.gz (of link group editor) doesn't exist
#11 7.827 update-alternatives: warning: skip creation of /usr/share/man/de/man1/editor.1.gz because associated file /usr/share/man/de/man1/vim.1.gz (of link group editor) doesn't exist
#11 7.827 update-alternatives: warning: skip creation of /usr/share/man/fr/man1/editor.1.gz because associated file /usr/share/man/fr/man1/vim.1.gz (of link group editor) doesn't exist
#11 7.827 update-alternatives: warning: skip creation of /usr/share/man/it/man1/editor.1.gz because associated file /usr/share/man/it/man1/vim.1.gz (of link group editor) doesn't exist
#11 7.827 update-alternatives: warning: skip creation of /usr/share/man/ja/man1/editor.1.gz because associated file /usr/share/man/ja/man1/vim.1.gz (of link group editor) doesn't exist
#11 7.827 update-alternatives: warning: skip creation of /usr/share/man/pl/man1/editor.1.gz because associated file /usr/share/man/pl/man1/vim.1.gz (of link group editor) doesn't exist
#11 7.827 update-alternatives: warning: skip creation of /usr/share/man/ru/man1/editor.1.gz because associated file /usr/share/man/ru/man1/vim.1.gz (of link group editor) doesn't exist
#11 7.827 update-alternatives: warning: skip creation of /usr/share/man/man1/editor.1.gz because associated file /usr/share/man/man1/vim.1.gz (of link group editor) doesn't exist
#11 7.862 Processing triggers for libc-bin (2.35-0ubuntu3.4) ...
#11 8.018 Cloning into 'apex'...
#11 12.06 No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'
#11 12.07 
#11 12.07 Warning: Torch did not find available GPUs on this system.
#11 12.07  If your intention is to cross-compile, this is not an error.
#11 12.07 By default, Apex will cross-compile for Pascal (compute capabilities 6.0, 6.1, 6.2),
#11 12.07 Volta (compute capability 7.0), Turing (compute capability 7.5),
#11 12.07 and, if the CUDA version is >= 11.0, Ampere (compute capability 8.0).
#11 12.07 If you wish to cross-compile for a single specific architecture,
#11 12.07 export TORCH_CUDA_ARCH_LIST="compute capability" before running setup.py.
#11 12.07 
#11 12.07 
#11 12.07 
#11 12.07 torch.__version__  = 2.1.2+cu121
#11 12.07 
#11 12.07 
#11 12.07 
#11 12.07 Compiling cuda extensions with
#11 12.07 nvcc: NVIDIA (R) Cuda compiler driver
#11 12.07 Copyright (c) 2005-2023 NVIDIA Corporation
#11 12.07 Built on Mon_Apr__3_17:16:06_PDT_2023
#11 12.07 Cuda compilation tools, release 12.1, V12.1.105
#11 12.07 Build cuda_12.1.r12.1/compiler.32688072_0
#11 12.07 from /usr/local/cuda/bin
#11 12.07 
#11 12.07 running install
#11 12.07 /usr/lib/python3/dist-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.
#11 12.07   warnings.warn(
#11 12.08 /usr/lib/python3/dist-packages/setuptools/command/easy_install.py:158: EasyInstallDeprecationWarning: easy_install command is deprecated. Use build and pip and other standards-based tools.
#11 12.08   warnings.warn(
#11 12.14 running bdist_egg
#11 12.15 running egg_info
#11 12.15 creating apex.egg-info
#11 12.15 writing apex.egg-info/PKG-INFO
#11 12.15 writing dependency_links to apex.egg-info/dependency_links.txt
#11 12.15 writing requirements to apex.egg-info/requires.txt
#11 12.15 writing top-level names to apex.egg-info/top_level.txt
#11 12.15 writing manifest file 'apex.egg-info/SOURCES.txt'
#11 12.18 reading manifest file 'apex.egg-info/SOURCES.txt'
#11 12.18 adding license file 'LICENSE'
#11 12.18 writing manifest file 'apex.egg-info/SOURCES.txt'
#11 12.18 installing library code to build/bdist.linux-x86_64/egg
#11 12.18 running install_lib
#11 12.18 running build_py
#11 12.18 creating build
#11 12.18 creating build/lib.linux-x86_64-3.10
#11 12.18 creating build/lib.linux-x86_64-3.10/apex
#11 12.18 copying apex/__init__.py -> build/lib.linux-x86_64-3.10/apex
#11 12.18 copying apex/_autocast_utils.py -> build/lib.linux-x86_64-3.10/apex
#11 12.18 creating build/lib.linux-x86_64-3.10/apex/fused_dense
#11 12.18 copying apex/fused_dense/__init__.py -> build/lib.linux-x86_64-3.10/apex/fused_dense
#11 12.18 copying apex/fused_dense/fused_dense.py -> build/lib.linux-x86_64-3.10/apex/fused_dense
#11 12.18 creating build/lib.linux-x86_64-3.10/apex/normalization
#11 12.18 copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.10/apex/normalization
#11 12.18 copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.10/apex/normalization
#11 12.18 creating build/lib.linux-x86_64-3.10/apex/amp
#11 12.18 copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.10/apex/amp
#11 12.18 copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.10/apex/amp
#11 12.19 copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.10/apex/amp
#11 12.19 copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.10/apex/amp
#11 12.19 copying apex/amp/utils.py -> build/lib.linux-x86_64-3.10/apex/amp
#11 12.19 copying apex/amp/opt.py -> build/lib.linux-x86_64-3.10/apex/amp
#11 12.19 copying apex/amp/handle.py -> build/lib.linux-x86_64-3.10/apex/amp
#11 12.19 copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.10/apex/amp
#11 12.19 copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.10/apex/amp
#11 12.19 copying apex/amp/compat.py -> build/lib.linux-x86_64-3.10/apex/amp
#11 12.19 copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.10/apex/amp
#11 12.19 copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.10/apex/amp
#11 12.19 copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.10/apex/amp
#11 12.19 copying apex/amp/amp.py -> build/lib.linux-x86_64-3.10/apex/amp
#11 12.19 creating build/lib.linux-x86_64-3.10/apex/fp16_utils
#11 12.19 copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.10/apex/fp16_utils
#11 12.19 copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.10/apex/fp16_utils
#11 12.19 copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.10/apex/fp16_utils
#11 12.19 copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.10/apex/fp16_utils
#11 12.19 creating build/lib.linux-x86_64-3.10/apex/parallel
#11 12.19 copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.10/apex/parallel
#11 12.19 copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.10/apex/parallel
#11 12.19 copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.10/apex/parallel
#11 12.19 copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.10/apex/parallel
#11 12.19 copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.10/apex/parallel
#11 12.19 copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.10/apex/parallel
#11 12.19 copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.10/apex/parallel
#11 12.19 copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.10/apex/parallel
#11 12.19 creating build/lib.linux-x86_64-3.10/apex/optimizers
#11 12.19 copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.10/apex/optimizers
#11 12.19 copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.10/apex/optimizers
#11 12.19 copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-3.10/apex/optimizers
#11 12.19 copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.10/apex/optimizers
#11 12.19 copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-3.10/apex/optimizers
#11 12.19 copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.10/apex/optimizers
#11 12.19 copying apex/optimizers/fused_mixed_precision_lamb.py -> build/lib.linux-x86_64-3.10/apex/optimizers
#11 12.19 creating build/lib.linux-x86_64-3.10/apex/multi_tensor_apply
#11 12.19 copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.10/apex/multi_tensor_apply
#11 12.19 copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.10/apex/multi_tensor_apply
#11 12.19 creating build/lib.linux-x86_64-3.10/apex/mlp
#11 12.19 copying apex/mlp/mlp.py -> build/lib.linux-x86_64-3.10/apex/mlp
#11 12.19 copying apex/mlp/__init__.py -> build/lib.linux-x86_64-3.10/apex/mlp
#11 12.19 creating build/lib.linux-x86_64-3.10/apex/contrib
#11 12.19 copying apex/contrib/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib
#11 12.19 creating build/lib.linux-x86_64-3.10/apex/transformer
#11 12.19 copying apex/transformer/enums.py -> build/lib.linux-x86_64-3.10/apex/transformer
#11 12.19 copying apex/transformer/parallel_state.py -> build/lib.linux-x86_64-3.10/apex/transformer
#11 12.19 copying apex/transformer/log_util.py -> build/lib.linux-x86_64-3.10/apex/transformer
#11 12.19 copying apex/transformer/__init__.py -> build/lib.linux-x86_64-3.10/apex/transformer
#11 12.19 copying apex/transformer/utils.py -> build/lib.linux-x86_64-3.10/apex/transformer
#11 12.19 copying apex/transformer/_ucc_util.py -> build/lib.linux-x86_64-3.10/apex/transformer
#11 12.19 copying apex/transformer/microbatches.py -> build/lib.linux-x86_64-3.10/apex/transformer
#11 12.19 creating build/lib.linux-x86_64-3.10/apex/RNN
#11 12.20 copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.10/apex/RNN
#11 12.20 copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.10/apex/RNN
#11 12.20 copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.10/apex/RNN
#11 12.20 copying apex/RNN/models.py -> build/lib.linux-x86_64-3.10/apex/RNN
#11 12.20 creating build/lib.linux-x86_64-3.10/apex/amp/lists
#11 12.20 copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.10/apex/amp/lists
#11 12.20 copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.10/apex/amp/lists
#11 12.20 copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.10/apex/amp/lists
#11 12.20 copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.10/apex/amp/lists
#11 12.20 creating build/lib.linux-x86_64-3.10/apex/contrib/bottleneck
#11 12.20 copying apex/contrib/bottleneck/test.py -> build/lib.linux-x86_64-3.10/apex/contrib/bottleneck
#11 12.20 copying apex/contrib/bottleneck/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib/bottleneck
#11 12.20 copying apex/contrib/bottleneck/bottleneck.py -> build/lib.linux-x86_64-3.10/apex/contrib/bottleneck
#11 12.20 copying apex/contrib/bottleneck/halo_exchangers.py -> build/lib.linux-x86_64-3.10/apex/contrib/bottleneck
#11 12.20 creating build/lib.linux-x86_64-3.10/apex/contrib/test
#11 12.20 copying apex/contrib/test/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib/test
#11 12.20 creating build/lib.linux-x86_64-3.10/apex/contrib/groupbn
#11 12.20 copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib/groupbn
#11 12.20 copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-3.10/apex/contrib/groupbn
#11 12.20 creating build/lib.linux-x86_64-3.10/apex/contrib/focal_loss
#11 12.20 copying apex/contrib/focal_loss/focal_loss.py -> build/lib.linux-x86_64-3.10/apex/contrib/focal_loss
#11 12.20 copying apex/contrib/focal_loss/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib/focal_loss
#11 12.20 creating build/lib.linux-x86_64-3.10/apex/contrib/fmha
#11 12.20 copying apex/contrib/fmha/fmha.py -> build/lib.linux-x86_64-3.10/apex/contrib/fmha
#11 12.20 copying apex/contrib/fmha/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib/fmha
#11 12.20 creating build/lib.linux-x86_64-3.10/apex/contrib/layer_norm
#11 12.20 copying apex/contrib/layer_norm/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib/layer_norm
#11 12.20 copying apex/contrib/layer_norm/layer_norm.py -> build/lib.linux-x86_64-3.10/apex/contrib/layer_norm
#11 12.20 creating build/lib.linux-x86_64-3.10/apex/contrib/cudnn_gbn
#11 12.20 copying apex/contrib/cudnn_gbn/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib/cudnn_gbn
#11 12.20 copying apex/contrib/cudnn_gbn/batch_norm.py -> build/lib.linux-x86_64-3.10/apex/contrib/cudnn_gbn
#11 12.20 creating build/lib.linux-x86_64-3.10/apex/contrib/conv_bias_relu
#11 12.20 copying apex/contrib/conv_bias_relu/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib/conv_bias_relu
#11 12.20 copying apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/lib.linux-x86_64-3.10/apex/contrib/conv_bias_relu
#11 12.20 creating build/lib.linux-x86_64-3.10/apex/contrib/optimizers
#11 12.20 copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-3.10/apex/contrib/optimizers
#11 12.20 copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib.linux-x86_64-3.10/apex/contrib/optimizers
#11 12.20 copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.10/apex/contrib/optimizers
#11 12.20 copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.10/apex/contrib/optimizers
#11 12.20 copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib/optimizers
#11 12.20 copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.10/apex/contrib/optimizers
#11 12.20 copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.10/apex/contrib/optimizers
#11 12.20 creating build/lib.linux-x86_64-3.10/apex/contrib/peer_memory
#11 12.20 copying apex/contrib/peer_memory/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib/peer_memory
#11 12.20 copying apex/contrib/peer_memory/peer_memory.py -> build/lib.linux-x86_64-3.10/apex/contrib/peer_memory
#11 12.20 copying apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/lib.linux-x86_64-3.10/apex/contrib/peer_memory
#11 12.20 creating build/lib.linux-x86_64-3.10/apex/contrib/index_mul_2d
#11 12.20 copying apex/contrib/index_mul_2d/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib/index_mul_2d
#11 12.20 copying apex/contrib/index_mul_2d/index_mul_2d.py -> build/lib.linux-x86_64-3.10/apex/contrib/index_mul_2d
#11 12.21 creating build/lib.linux-x86_64-3.10/apex/contrib/multihead_attn
#11 12.21 copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.10/apex/contrib/multihead_attn
#11 12.21 copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-3.10/apex/contrib/multihead_attn
#11 12.21 copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.10/apex/contrib/multihead_attn
#11 12.21 copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-3.10/apex/contrib/multihead_attn
#11 12.21 copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib/multihead_attn
#11 12.21 copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-3.10/apex/contrib/multihead_attn
#11 12.21 copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.10/apex/contrib/multihead_attn
#11 12.21 copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-3.10/apex/contrib/multihead_attn
#11 12.21 copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib.linux-x86_64-3.10/apex/contrib/multihead_attn
#11 12.21 copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.10/apex/contrib/multihead_attn
#11 12.21 creating build/lib.linux-x86_64-3.10/apex/contrib/group_norm
#11 12.21 copying apex/contrib/group_norm/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib/group_norm
#11 12.21 copying apex/contrib/group_norm/group_norm.py -> build/lib.linux-x86_64-3.10/apex/contrib/group_norm
#11 12.21 creating build/lib.linux-x86_64-3.10/apex/contrib/sparsity
#11 12.21 copying apex/contrib/sparsity/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib/sparsity
#11 12.21 copying apex/contrib/sparsity/sparse_masklib.py -> build/lib.linux-x86_64-3.10/apex/contrib/sparsity
#11 12.21 copying apex/contrib/sparsity/permutation_lib.py -> build/lib.linux-x86_64-3.10/apex/contrib/sparsity
#11 12.21 copying apex/contrib/sparsity/asp.py -> build/lib.linux-x86_64-3.10/apex/contrib/sparsity
#11 12.21 creating build/lib.linux-x86_64-3.10/apex/contrib/openfold_triton
#11 12.21 copying apex/contrib/openfold_triton/_mha_kernel.py -> build/lib.linux-x86_64-3.10/apex/contrib/openfold_triton
#11 12.21 copying apex/contrib/openfold_triton/fused_adam_swa.py -> build/lib.linux-x86_64-3.10/apex/contrib/openfold_triton
#11 12.21 copying apex/contrib/openfold_triton/_layer_norm_config_ampere.py -> build/lib.linux-x86_64-3.10/apex/contrib/openfold_triton
#11 12.21 copying apex/contrib/openfold_triton/mha.py -> build/lib.linux-x86_64-3.10/apex/contrib/openfold_triton
#11 12.21 copying apex/contrib/openfold_triton/_layer_norm_backward_kernels.py -> build/lib.linux-x86_64-3.10/apex/contrib/openfold_triton
#11 12.21 copying apex/contrib/openfold_triton/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib/openfold_triton
#11 12.21 copying apex/contrib/openfold_triton/_layer_norm_forward_kernels.py -> build/lib.linux-x86_64-3.10/apex/contrib/openfold_triton
#11 12.21 copying apex/contrib/openfold_triton/_layer_norm_config_hopper.py -> build/lib.linux-x86_64-3.10/apex/contrib/openfold_triton
#11 12.21 copying apex/contrib/openfold_triton/layer_norm.py -> build/lib.linux-x86_64-3.10/apex/contrib/openfold_triton
#11 12.21 creating build/lib.linux-x86_64-3.10/apex/contrib/transducer
#11 12.21 copying apex/contrib/transducer/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib/transducer
#11 12.21 copying apex/contrib/transducer/_transducer_ref.py -> build/lib.linux-x86_64-3.10/apex/contrib/transducer
#11 12.21 copying apex/contrib/transducer/transducer.py -> build/lib.linux-x86_64-3.10/apex/contrib/transducer
#11 12.21 creating build/lib.linux-x86_64-3.10/apex/contrib/clip_grad
#11 12.21 copying apex/contrib/clip_grad/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib/clip_grad
#11 12.21 copying apex/contrib/clip_grad/clip_grad.py -> build/lib.linux-x86_64-3.10/apex/contrib/clip_grad
#11 12.21 creating build/lib.linux-x86_64-3.10/apex/contrib/xentropy
#11 12.21 copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-3.10/apex/contrib/xentropy
#11 12.21 copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib/xentropy
#11 12.21 creating build/lib.linux-x86_64-3.10/apex/contrib/test/bottleneck
#11 12.21 copying apex/contrib/test/bottleneck/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/bottleneck
#11 12.21 copying apex/contrib/test/bottleneck/test_bottleneck_module.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/bottleneck
#11 12.21 creating build/lib.linux-x86_64-3.10/apex/contrib/test/focal_loss
#11 12.21 copying apex/contrib/test/focal_loss/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/focal_loss
#11 12.21 copying apex/contrib/test/focal_loss/test_focal_loss.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/focal_loss
#11 12.21 creating build/lib.linux-x86_64-3.10/apex/contrib/test/fmha
#11 12.21 copying apex/contrib/test/fmha/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/fmha
#11 12.21 copying apex/contrib/test/fmha/test_fmha.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/fmha
#11 12.21 creating build/lib.linux-x86_64-3.10/apex/contrib/test/layer_norm
#11 12.21 copying apex/contrib/test/layer_norm/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/layer_norm
#11 12.22 copying apex/contrib/test/layer_norm/test_fast_layer_norm.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/layer_norm
#11 12.22 creating build/lib.linux-x86_64-3.10/apex/contrib/test/cudnn_gbn
#11 12.22 copying apex/contrib/test/cudnn_gbn/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/cudnn_gbn
#11 12.22 copying apex/contrib/test/cudnn_gbn/test_cudnn_gbn_with_two_gpus.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/cudnn_gbn
#11 12.22 creating build/lib.linux-x86_64-3.10/apex/contrib/test/conv_bias_relu
#11 12.22 copying apex/contrib/test/conv_bias_relu/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/conv_bias_relu
#11 12.22 copying apex/contrib/test/conv_bias_relu/test_conv_bias_relu.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/conv_bias_relu
#11 12.22 creating build/lib.linux-x86_64-3.10/apex/contrib/test/optimizers
#11 12.22 copying apex/contrib/test/optimizers/test_distributed_fused_lamb.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/optimizers
#11 12.22 copying apex/contrib/test/optimizers/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/optimizers
#11 12.22 copying apex/contrib/test/optimizers/test_dist_adam.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/optimizers
#11 12.22 creating build/lib.linux-x86_64-3.10/apex/contrib/test/peer_memory
#11 12.22 copying apex/contrib/test/peer_memory/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/peer_memory
#11 12.22 copying apex/contrib/test/peer_memory/test_peer_halo_exchange_module.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/peer_memory
#11 12.22 creating build/lib.linux-x86_64-3.10/apex/contrib/test/index_mul_2d
#11 12.22 copying apex/contrib/test/index_mul_2d/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/index_mul_2d
#11 12.22 copying apex/contrib/test/index_mul_2d/test_index_mul_2d.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/index_mul_2d
#11 12.22 creating build/lib.linux-x86_64-3.10/apex/contrib/test/multihead_attn
#11 12.22 copying apex/contrib/test/multihead_attn/test_fast_self_multihead_attn_bias.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/multihead_attn
#11 12.22 copying apex/contrib/test/multihead_attn/test_mha_fused_softmax.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/multihead_attn
#11 12.22 copying apex/contrib/test/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/multihead_attn
#11 12.22 copying apex/contrib/test/multihead_attn/test_self_multihead_attn.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/multihead_attn
#11 12.22 copying apex/contrib/test/multihead_attn/test_encdec_multihead_attn.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/multihead_attn
#11 12.22 copying apex/contrib/test/multihead_attn/test_encdec_multihead_attn_norm_add.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/multihead_attn
#11 12.22 copying apex/contrib/test/multihead_attn/test_self_multihead_attn_norm_add.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/multihead_attn
#11 12.22 creating build/lib.linux-x86_64-3.10/apex/contrib/test/group_norm
#11 12.22 copying apex/contrib/test/group_norm/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/group_norm
#11 12.22 copying apex/contrib/test/group_norm/test_group_norm.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/group_norm
#11 12.22 creating build/lib.linux-x86_64-3.10/apex/contrib/test/transducer
#11 12.22 copying apex/contrib/test/transducer/test_transducer_loss.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/transducer
#11 12.22 copying apex/contrib/test/transducer/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/transducer
#11 12.22 copying apex/contrib/test/transducer/test_transducer_joint.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/transducer
#11 12.22 creating build/lib.linux-x86_64-3.10/apex/contrib/test/clip_grad
#11 12.22 copying apex/contrib/test/clip_grad/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/clip_grad
#11 12.22 copying apex/contrib/test/clip_grad/test_clip_grad.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/clip_grad
#11 12.22 creating build/lib.linux-x86_64-3.10/apex/contrib/test/xentropy
#11 12.22 copying apex/contrib/test/xentropy/test_label_smoothing.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/xentropy
#11 12.22 copying apex/contrib/test/xentropy/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib/test/xentropy
#11 12.22 creating build/lib.linux-x86_64-3.10/apex/contrib/sparsity/permutation_search_kernels
#11 12.22 copying apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/lib.linux-x86_64-3.10/apex/contrib/sparsity/permutation_search_kernels
#11 12.22 copying apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/lib.linux-x86_64-3.10/apex/contrib/sparsity/permutation_search_kernels
#11 12.22 copying apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/lib.linux-x86_64-3.10/apex/contrib/sparsity/permutation_search_kernels
#11 12.22 copying apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/lib.linux-x86_64-3.10/apex/contrib/sparsity/permutation_search_kernels
#11 12.22 copying apex/contrib/sparsity/permutation_search_kernels/channel_swap.py -> build/lib.linux-x86_64-3.10/apex/contrib/sparsity/permutation_search_kernels
#11 12.22 creating build/lib.linux-x86_64-3.10/apex/transformer/functional
#11 12.22 copying apex/transformer/functional/fused_softmax.py -> build/lib.linux-x86_64-3.10/apex/transformer/functional
#11 12.22 copying apex/transformer/functional/__init__.py -> build/lib.linux-x86_64-3.10/apex/transformer/functional
#11 12.22 copying apex/transformer/functional/fused_rope.py -> build/lib.linux-x86_64-3.10/apex/transformer/functional
#11 12.22 creating build/lib.linux-x86_64-3.10/apex/transformer/amp
#11 12.23 copying apex/transformer/amp/__init__.py -> build/lib.linux-x86_64-3.10/apex/transformer/amp
#11 12.23 copying apex/transformer/amp/grad_scaler.py -> build/lib.linux-x86_64-3.10/apex/transformer/amp
#11 12.23 creating build/lib.linux-x86_64-3.10/apex/transformer/_data
#11 12.23 copying apex/transformer/_data/__init__.py -> build/lib.linux-x86_64-3.10/apex/transformer/_data
#11 12.23 copying apex/transformer/_data/_batchsampler.py -> build/lib.linux-x86_64-3.10/apex/transformer/_data
#11 12.23 creating build/lib.linux-x86_64-3.10/apex/transformer/tensor_parallel
#11 12.23 copying apex/transformer/tensor_parallel/memory.py -> build/lib.linux-x86_64-3.10/apex/transformer/tensor_parallel
#11 12.23 copying apex/transformer/tensor_parallel/cross_entropy.py -> build/lib.linux-x86_64-3.10/apex/transformer/tensor_parallel
#11 12.23 copying apex/transformer/tensor_parallel/__init__.py -> build/lib.linux-x86_64-3.10/apex/transformer/tensor_parallel
#11 12.23 copying apex/transformer/tensor_parallel/utils.py -> build/lib.linux-x86_64-3.10/apex/transformer/tensor_parallel
#11 12.23 copying apex/transformer/tensor_parallel/data.py -> build/lib.linux-x86_64-3.10/apex/transformer/tensor_parallel
#11 12.23 copying apex/transformer/tensor_parallel/mappings.py -> build/lib.linux-x86_64-3.10/apex/transformer/tensor_parallel
#11 12.23 copying apex/transformer/tensor_parallel/random.py -> build/lib.linux-x86_64-3.10/apex/transformer/tensor_parallel
#11 12.23 copying apex/transformer/tensor_parallel/layers.py -> build/lib.linux-x86_64-3.10/apex/transformer/tensor_parallel
#11 12.23 creating build/lib.linux-x86_64-3.10/apex/transformer/layers
#11 12.23 copying apex/transformer/layers/__init__.py -> build/lib.linux-x86_64-3.10/apex/transformer/layers
#11 12.23 copying apex/transformer/layers/layer_norm.py -> build/lib.linux-x86_64-3.10/apex/transformer/layers
#11 12.23 creating build/lib.linux-x86_64-3.10/apex/transformer/testing
#11 12.23 copying apex/transformer/testing/commons.py -> build/lib.linux-x86_64-3.10/apex/transformer/testing
#11 12.23 copying apex/transformer/testing/standalone_bert.py -> build/lib.linux-x86_64-3.10/apex/transformer/testing
#11 12.23 copying apex/transformer/testing/standalone_gpt.py -> build/lib.linux-x86_64-3.10/apex/transformer/testing
#11 12.23 copying apex/transformer/testing/__init__.py -> build/lib.linux-x86_64-3.10/apex/transformer/testing
#11 12.23 copying apex/transformer/testing/distributed_test_base.py -> build/lib.linux-x86_64-3.10/apex/transformer/testing
#11 12.23 copying apex/transformer/testing/standalone_transformer_lm.py -> build/lib.linux-x86_64-3.10/apex/transformer/testing
#11 12.23 copying apex/transformer/testing/global_vars.py -> build/lib.linux-x86_64-3.10/apex/transformer/testing
#11 12.23 copying apex/transformer/testing/arguments.py -> build/lib.linux-x86_64-3.10/apex/transformer/testing
#11 12.23 creating build/lib.linux-x86_64-3.10/apex/transformer/pipeline_parallel
#11 12.23 copying apex/transformer/pipeline_parallel/p2p_communication.py -> build/lib.linux-x86_64-3.10/apex/transformer/pipeline_parallel
#11 12.23 copying apex/transformer/pipeline_parallel/__init__.py -> build/lib.linux-x86_64-3.10/apex/transformer/pipeline_parallel
#11 12.23 copying apex/transformer/pipeline_parallel/utils.py -> build/lib.linux-x86_64-3.10/apex/transformer/pipeline_parallel
#11 12.23 copying apex/transformer/pipeline_parallel/_timers.py -> build/lib.linux-x86_64-3.10/apex/transformer/pipeline_parallel
#11 12.23 creating build/lib.linux-x86_64-3.10/apex/transformer/pipeline_parallel/schedules
#11 12.23 copying apex/transformer/pipeline_parallel/schedules/common.py -> build/lib.linux-x86_64-3.10/apex/transformer/pipeline_parallel/schedules
#11 12.23 copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/lib.linux-x86_64-3.10/apex/transformer/pipeline_parallel/schedules
#11 12.23 copying apex/transformer/pipeline_parallel/schedules/__init__.py -> build/lib.linux-x86_64-3.10/apex/transformer/pipeline_parallel/schedules
#11 12.23 copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/lib.linux-x86_64-3.10/apex/transformer/pipeline_parallel/schedules
#11 12.23 copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/lib.linux-x86_64-3.10/apex/transformer/pipeline_parallel/schedules
#11 12.24 running build_ext
#11 12.24 /usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:424: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.1
#11 12.24   warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')
#11 12.24 building 'amp_C' extension
#11 12.24 creating /app/apex/build/temp.linux-x86_64-3.10
#11 12.24 creating /app/apex/build/temp.linux-x86_64-3.10/csrc
#11 12.27 Emitting ninja build file /app/apex/build/temp.linux-x86_64-3.10/build.ninja...
#11 12.27 Compiling objects...
#11 12.27 Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
#11 69.56 [1/15] c++ -MMD -MF /app/apex/build/temp.linux-x86_64-3.10/csrc/amp_C_frontend.o.d -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c -c /app/apex/csrc/amp_C_frontend.cpp -o /app/apex/build/temp.linux-x86_64-3.10/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
#11 177.5 [2/15] /usr/local/cuda/bin/nvcc  -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c -c /app/apex/csrc/multi_tensor_adagrad.cu -o /app/apex/build/temp.linux-x86_64-3.10/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_62,code=sm_62 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_90,code=sm_90 -std=c++17
#11 178.8 [3/15] /usr/local/cuda/bin/nvcc  -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c -c /app/apex/csrc/multi_tensor_l2norm_kernel_mp.cu -o /app/apex/build/temp.linux-x86_64-3.10/csrc/multi_tensor_l2norm_kernel_mp.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_62,code=sm_62 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_90,code=sm_90 -std=c++17
#11 181.8 [4/15] /usr/local/cuda/bin/nvcc  -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c -c /app/apex/csrc/multi_tensor_l2norm_kernel.cu -o /app/apex/build/temp.linux-x86_64-3.10/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_62,code=sm_62 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_90,code=sm_90 -std=c++17
#11 187.8 [5/15] /usr/local/cuda/bin/nvcc  -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c -c /app/apex/csrc/multi_tensor_axpby_kernel.cu -o /app/apex/build/temp.linux-x86_64-3.10/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_62,code=sm_62 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_90,code=sm_90 -std=c++17
#11 193.3 [6/15] /usr/local/cuda/bin/nvcc  -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c -c /app/apex/csrc/multi_tensor_adam.cu -o /app/apex/build/temp.linux-x86_64-3.10/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_62,code=sm_62 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_90,code=sm_90 -std=c++17
#11 240.6 [7/15] /usr/local/cuda/bin/nvcc  -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c -c /app/apex/csrc/multi_tensor_l2norm_scale_kernel.cu -o /app/apex/build/temp.linux-x86_64-3.10/csrc/multi_tensor_l2norm_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_62,code=sm_62 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_90,code=sm_90 -std=c++17
#11 344.2 [8/15] /usr/local/cuda/bin/nvcc  -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c -c /app/apex/csrc/multi_tensor_lamb.cu -o /app/apex/build/temp.linux-x86_64-3.10/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_62,code=sm_62 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_90,code=sm_90 -std=c++17
#11 347.7 [9/15] /usr/local/cuda/bin/nvcc  -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c -c /app/apex/csrc/multi_tensor_lamb_stage_1.cu -o /app/apex/build/temp.linux-x86_64-3.10/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_62,code=sm_62 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_90,code=sm_90 -std=c++17
#11 351.9 [10/15] /usr/local/cuda/bin/nvcc  -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c -c /app/apex/csrc/multi_tensor_lamb_stage_2.cu -o /app/apex/build/temp.linux-x86_64-3.10/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_62,code=sm_62 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_90,code=sm_90 -std=c++17
#11 352.1 [11/15] /usr/local/cuda/bin/nvcc  -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c -c /app/apex/csrc/multi_tensor_lamb_mp.cu -o /app/apex/build/temp.linux-x86_64-3.10/csrc/multi_tensor_lamb_mp.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_62,code=sm_62 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_90,code=sm_90 -std=c++17
#11 356.9 [12/15] /usr/local/cuda/bin/nvcc  -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c -c /app/apex/csrc/multi_tensor_novograd.cu -o /app/apex/build/temp.linux-x86_64-3.10/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_62,code=sm_62 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_90,code=sm_90 -std=c++17
#11 385.6 [13/15] /usr/local/cuda/bin/nvcc  -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c -c /app/apex/csrc/multi_tensor_scale_kernel.cu -o /app/apex/build/temp.linux-x86_64-3.10/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_62,code=sm_62 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_90,code=sm_90 -std=c++17
#11 426.7 [14/15] /usr/local/cuda/bin/nvcc  -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c -c /app/apex/csrc/update_scale_hysteresis.cu -o /app/apex/build/temp.linux-x86_64-3.10/csrc/update_scale_hysteresis.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_62,code=sm_62 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_90,code=sm_90 -std=c++17
#11 430.8 [15/15] /usr/local/cuda/bin/nvcc  -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c -c /app/apex/csrc/multi_tensor_sgd_kernel.cu -o /app/apex/build/temp.linux-x86_64-3.10/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_62,code=sm_62 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_90,code=sm_90 -std=c++17
#11 430.8 x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 /app/apex/build/temp.linux-x86_64-3.10/csrc/amp_C_frontend.o /app/apex/build/temp.linux-x86_64-3.10/csrc/multi_tensor_adagrad.o /app/apex/build/temp.linux-x86_64-3.10/csrc/multi_tensor_adam.o /app/apex/build/temp.linux-x86_64-3.10/csrc/multi_tensor_axpby_kernel.o /app/apex/build/temp.linux-x86_64-3.10/csrc/multi_tensor_l2norm_kernel.o /app/apex/build/temp.linux-x86_64-3.10/csrc/multi_tensor_l2norm_kernel_mp.o /app/apex/build/temp.linux-x86_64-3.10/csrc/multi_tensor_l2norm_scale_kernel.o /app/apex/build/temp.linux-x86_64-3.10/csrc/multi_tensor_lamb.o /app/apex/build/temp.linux-x86_64-3.10/csrc/multi_tensor_lamb_mp.o /app/apex/build/temp.linux-x86_64-3.10/csrc/multi_tensor_lamb_stage_1.o /app/apex/build/temp.linux-x86_64-3.10/csrc/multi_tensor_lamb_stage_2.o /app/apex/build/temp.linux-x86_64-3.10/csrc/multi_tensor_novograd.o /app/apex/build/temp.linux-x86_64-3.10/csrc/multi_tensor_scale_kernel.o /app/apex/build/temp.linux-x86_64-3.10/csrc/multi_tensor_sgd_kernel.o /app/apex/build/temp.linux-x86_64-3.10/csrc/update_scale_hysteresis.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.10/amp_C.cpython-310-x86_64-linux-gnu.so
#11 431.2 building 'syncbn' extension
#11 431.3 Emitting ninja build file /app/apex/build/temp.linux-x86_64-3.10/build.ninja...
#11 431.3 Compiling objects...
#11 431.3 Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
#11 453.9 [1/2] c++ -MMD -MF /app/apex/build/temp.linux-x86_64-3.10/csrc/syncbn.o.d -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c -c /app/apex/csrc/syncbn.cpp -o /app/apex/build/temp.linux-x86_64-3.10/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
#11 513.7 [2/2] /usr/local/cuda/bin/nvcc  -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c -c /app/apex/csrc/welford.cu -o /app/apex/build/temp.linux-x86_64-3.10/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_62,code=sm_62 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_90,code=sm_90 -std=c++17
#11 513.8 x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 /app/apex/build/temp.linux-x86_64-3.10/csrc/syncbn.o /app/apex/build/temp.linux-x86_64-3.10/csrc/welford.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.10/syncbn.cpython-310-x86_64-linux-gnu.so
#11 514.1 building 'fused_layer_norm_cuda' extension
#11 514.1 Emitting ninja build file /app/apex/build/temp.linux-x86_64-3.10/build.ninja...
#11 514.1 Compiling objects...
#11 514.1 Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
#11 537.8 [1/2] c++ -MMD -MF /app/apex/build/temp.linux-x86_64-3.10/csrc/layer_norm_cuda.o.d -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c -c /app/apex/csrc/layer_norm_cuda.cpp -o /app/apex/build/temp.linux-x86_64-3.10/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
#11 639.2 [2/2] /usr/local/cuda/bin/nvcc  -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c -c /app/apex/csrc/layer_norm_cuda_kernel.cu -o /app/apex/build/temp.linux-x86_64-3.10/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_62,code=sm_62 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_90,code=sm_90 -std=c++17
#11 639.2 x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 /app/apex/build/temp.linux-x86_64-3.10/csrc/layer_norm_cuda.o /app/apex/build/temp.linux-x86_64-3.10/csrc/layer_norm_cuda_kernel.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.10/fused_layer_norm_cuda.cpython-310-x86_64-linux-gnu.so
#11 639.6 building 'mlp_cuda' extension
#11 639.7 Emitting ninja build file /app/apex/build/temp.linux-x86_64-3.10/build.ninja...
#11 639.7 Compiling objects...
#11 639.7 Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
#11 662.1 [1/2] c++ -MMD -MF /app/apex/build/temp.linux-x86_64-3.10/csrc/mlp.o.d -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c -c /app/apex/csrc/mlp.cpp -o /app/apex/build/temp.linux-x86_64-3.10/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
#11 662.1 /app/apex/csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)’:
#11 662.1 /app/apex/csrc/mlp.cpp:57:21: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]
#11 662.1    57 |   for (int i = 0; i < num_layers; i++) {
#11 662.1       |                   ~~^~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp:64:76: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]
#11 662.1    64 |   auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());
#11 662.1       |                                                              ~~~~~~~~~~~~~~^~
#11 662.1 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 662.1                  from /app/apex/csrc/mlp.cpp:1:
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here
#11 662.1   225 |   DeprecatedTypeProperties & type() const {
#11 662.1       |                              ^~~~
#11 662.1 /app/apex/csrc/mlp.cpp:65:85: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]
#11 662.1    65 |   auto reserved_space = at::empty({static_cast<long>(reserved_size)}, inputs[0].type());
#11 662.1       |                                                                       ~~~~~~~~~~~~~~^~
#11 662.1 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 662.1                  from /app/apex/csrc/mlp.cpp:1:
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here
#11 662.1   225 |   DeprecatedTypeProperties & type() const {
#11 662.1       |                              ^~~~
#11 662.1 /app/apex/csrc/mlp.cpp:67:58: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]
#11 662.1    67 |   auto lt_workspace = at::empty({1 << 22}, inputs[0].type());
#11 662.1       |                                            ~~~~~~~~~~~~~~^~
#11 662.1 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 662.1                  from /app/apex/csrc/mlp.cpp:1:
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here
#11 662.1   225 |   DeprecatedTypeProperties & type() const {
#11 662.1       |                              ^~~~
#11 662.1 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 662.1                  from /app/apex/csrc/mlp.cpp:1:
#11 662.1 /app/apex/csrc/mlp.cpp: In lambda function:
#11 662.1 /app/apex/csrc/mlp.cpp:69:53: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]
#11 662.1    69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), "mlp_forward", [&] {
#11 662.1       |                                       ~~~~~~~~~~~~~~^~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:215:28: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 662.1   215 |     const auto& the_type = TYPE;                                            \
#11 662.1       |                            ^~~~
#11 662.1 /app/apex/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’
#11 662.1    69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), "mlp_forward", [&] {
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 662.1                  from /app/apex/csrc/mlp.cpp:1:
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here
#11 662.1   225 |   DeprecatedTypeProperties & type() const {
#11 662.1       |                              ^~~~
#11 662.1 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 662.1                  from /app/apex/csrc/mlp.cpp:1:
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:218:47: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]
#11 662.1   218 |     at::ScalarType _st = ::detail::scalar_type(the_type);                   \
#11 662.1       |                          ~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:245:3: note: in expansion of macro ‘AT_DISPATCH_SWITCH’
#11 662.1   245 |   AT_DISPATCH_SWITCH(                                        \
#11 662.1       |   ^~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’
#11 662.1    69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), "mlp_forward", [&] {
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:109:23: note: declared here
#11 662.1   109 | inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {
#11 662.1       |                       ^~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp: In lambda function:
#11 662.1 /app/apex/csrc/mlp.cpp:72:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]
#11 662.1    72 |     for (int i = 0; i < num_layers; i++) {
#11 662.1       |                     ~~^~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 662.1   221 |       __VA_ARGS__                                                           \
#11 662.1       |       ^~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 662.1    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:240:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 662.1   240 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \
#11 662.1       |   ^~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’
#11 662.1   246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))
#11 662.1       |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’
#11 662.1    69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), "mlp_forward", [&] {
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]
#11 662.1    78 |     auto result = mlp_fp<scalar_t>(
#11 662.1       |          ^~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 662.1   221 |       __VA_ARGS__                                                           \
#11 662.1       |       ^~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 662.1    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:240:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 662.1   240 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \
#11 662.1       |   ^~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’
#11 662.1   246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))
#11 662.1       |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’
#11 662.1    69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), "mlp_forward", [&] {
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp: In lambda function:
#11 662.1 /app/apex/csrc/mlp.cpp:72:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]
#11 662.1    72 |     for (int i = 0; i < num_layers; i++) {
#11 662.1       |                     ~~^~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 662.1   221 |       __VA_ARGS__                                                           \
#11 662.1       |       ^~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 662.1    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:241:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 662.1   241 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \
#11 662.1       |   ^~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’
#11 662.1   246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))
#11 662.1       |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’
#11 662.1    69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), "mlp_forward", [&] {
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]
#11 662.1    78 |     auto result = mlp_fp<scalar_t>(
#11 662.1       |          ^~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 662.1   221 |       __VA_ARGS__                                                           \
#11 662.1       |       ^~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 662.1    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:241:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 662.1   241 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \
#11 662.1       |   ^~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’
#11 662.1   246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))
#11 662.1       |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’
#11 662.1    69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), "mlp_forward", [&] {
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp: In lambda function:
#11 662.1 /app/apex/csrc/mlp.cpp:72:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]
#11 662.1    72 |     for (int i = 0; i < num_layers; i++) {
#11 662.1       |                     ~~^~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 662.1   221 |       __VA_ARGS__                                                           \
#11 662.1       |       ^~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 662.1    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:242:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 662.1   242 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)
#11 662.1       |   ^~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’
#11 662.1   246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))
#11 662.1       |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’
#11 662.1    69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), "mlp_forward", [&] {
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]
#11 662.1    78 |     auto result = mlp_fp<scalar_t>(
#11 662.1       |          ^~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 662.1   221 |       __VA_ARGS__                                                           \
#11 662.1       |       ^~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 662.1    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:242:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 662.1   242 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)
#11 662.1       |   ^~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’
#11 662.1   246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))
#11 662.1       |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’
#11 662.1    69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), "mlp_forward", [&] {
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)’:
#11 662.1 /app/apex/csrc/mlp.cpp:115:21: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]
#11 662.1   115 |   for (int i = 0; i < num_layers; i++) {
#11 662.1       |                   ~~^~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp:120:21: warning: comparison of integer expressions of different signedness: ‘int’ and ‘std::vector<at::Tensor>::size_type’ {aka ‘long unsigned int’} [-Wsign-compare]
#11 662.1   120 |   for (int i = 0; i < inputs.size(); i++) {
#11 662.1       |                   ~~^~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp:121:66: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]
#11 662.1   121 |     outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now
#11 662.1       |                                                    ~~~~~~~~~~~~~~^~
#11 662.1 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 662.1                  from /app/apex/csrc/mlp.cpp:1:
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here
#11 662.1   225 |   DeprecatedTypeProperties & type() const {
#11 662.1       |                              ^~~~
#11 662.1 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 662.1                  from /app/apex/csrc/mlp.cpp:1:
#11 662.1 /app/apex/csrc/mlp.cpp: In lambda function:
#11 662.1 /app/apex/csrc/mlp.cpp:124:53: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]
#11 662.1   124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), "mlp_backward", [&] {
#11 662.1       |                                       ~~~~~~~~~~~~~~^~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:215:28: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 662.1   215 |     const auto& the_type = TYPE;                                            \
#11 662.1       |                            ^~~~
#11 662.1 /app/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’
#11 662.1   124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), "mlp_backward", [&] {
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 662.1                  from /app/apex/csrc/mlp.cpp:1:
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here
#11 662.1   225 |   DeprecatedTypeProperties & type() const {
#11 662.1       |                              ^~~~
#11 662.1 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 662.1                  from /app/apex/csrc/mlp.cpp:1:
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:218:47: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]
#11 662.1   218 |     at::ScalarType _st = ::detail::scalar_type(the_type);                   \
#11 662.1       |                          ~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:245:3: note: in expansion of macro ‘AT_DISPATCH_SWITCH’
#11 662.1   245 |   AT_DISPATCH_SWITCH(                                        \
#11 662.1       |   ^~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’
#11 662.1   124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), "mlp_backward", [&] {
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:109:23: note: declared here
#11 662.1   109 | inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {
#11 662.1       |                       ^~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp: In lambda function:
#11 662.1 /app/apex/csrc/mlp.cpp:126:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]
#11 662.1   126 |     for (int i = 0; i < num_layers; i++) {
#11 662.1       |                     ~~^~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 662.1   221 |       __VA_ARGS__                                                           \
#11 662.1       |       ^~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 662.1    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:240:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 662.1   240 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \
#11 662.1       |   ^~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’
#11 662.1   246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))
#11 662.1       |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’
#11 662.1   124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), "mlp_backward", [&] {
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp:130:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘std::vector<at::Tensor>::size_type’ {aka ‘long unsigned int’} [-Wsign-compare]
#11 662.1   130 |     for (int i = 0; i < inputs.size(); i++) {
#11 662.1       |                     ~~^~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 662.1   221 |       __VA_ARGS__                                                           \
#11 662.1       |       ^~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 662.1    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:240:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 662.1   240 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \
#11 662.1       |   ^~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’
#11 662.1   246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))
#11 662.1       |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’
#11 662.1   124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), "mlp_backward", [&] {
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp:138:98: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]
#11 662.1   138 |     auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type());
#11 662.1       |                                                                                    ~~~~~~~~~~~~~~^~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 662.1   221 |       __VA_ARGS__                                                           \
#11 662.1       |       ^~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 662.1    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:240:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 662.1   240 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \
#11 662.1       |   ^~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’
#11 662.1   246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))
#11 662.1       |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’
#11 662.1   124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), "mlp_backward", [&] {
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 662.1                  from /app/apex/csrc/mlp.cpp:1:
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here
#11 662.1   225 |   DeprecatedTypeProperties & type() const {
#11 662.1       |                              ^~~~
#11 662.1 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 662.1                  from /app/apex/csrc/mlp.cpp:1:
#11 662.1 /app/apex/csrc/mlp.cpp:140:10: warning: unused variable ‘result’ [-Wunused-variable]
#11 662.1   140 |     auto result = mlp_bp<scalar_t>(
#11 662.1       |          ^~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 662.1   221 |       __VA_ARGS__                                                           \
#11 662.1       |       ^~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 662.1    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:240:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 662.1   240 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \
#11 662.1       |   ^~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’
#11 662.1   246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))
#11 662.1       |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’
#11 662.1   124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), "mlp_backward", [&] {
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp: In lambda function:
#11 662.1 /app/apex/csrc/mlp.cpp:126:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]
#11 662.1   126 |     for (int i = 0; i < num_layers; i++) {
#11 662.1       |                     ~~^~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 662.1   221 |       __VA_ARGS__                                                           \
#11 662.1       |       ^~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 662.1    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:241:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 662.1   241 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \
#11 662.1       |   ^~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’
#11 662.1   246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))
#11 662.1       |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’
#11 662.1   124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), "mlp_backward", [&] {
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp:130:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘std::vector<at::Tensor>::size_type’ {aka ‘long unsigned int’} [-Wsign-compare]
#11 662.1   130 |     for (int i = 0; i < inputs.size(); i++) {
#11 662.1       |                     ~~^~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 662.1   221 |       __VA_ARGS__                                                           \
#11 662.1       |       ^~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 662.1    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:241:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 662.1   241 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \
#11 662.1       |   ^~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’
#11 662.1   246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))
#11 662.1       |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’
#11 662.1   124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), "mlp_backward", [&] {
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp:138:98: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]
#11 662.1   138 |     auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type());
#11 662.1       |                                                                                    ~~~~~~~~~~~~~~^~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 662.1   221 |       __VA_ARGS__                                                           \
#11 662.1       |       ^~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 662.1    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:241:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 662.1   241 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \
#11 662.1       |   ^~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’
#11 662.1   246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))
#11 662.1       |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’
#11 662.1   124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), "mlp_backward", [&] {
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 662.1                  from /app/apex/csrc/mlp.cpp:1:
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here
#11 662.1   225 |   DeprecatedTypeProperties & type() const {
#11 662.1       |                              ^~~~
#11 662.1 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 662.1                  from /app/apex/csrc/mlp.cpp:1:
#11 662.1 /app/apex/csrc/mlp.cpp:140:10: warning: unused variable ‘result’ [-Wunused-variable]
#11 662.1   140 |     auto result = mlp_bp<scalar_t>(
#11 662.1       |          ^~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 662.1   221 |       __VA_ARGS__                                                           \
#11 662.1       |       ^~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 662.1    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:241:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 662.1   241 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \
#11 662.1       |   ^~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’
#11 662.1   246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))
#11 662.1       |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’
#11 662.1   124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), "mlp_backward", [&] {
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp: In lambda function:
#11 662.1 /app/apex/csrc/mlp.cpp:126:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]
#11 662.1   126 |     for (int i = 0; i < num_layers; i++) {
#11 662.1       |                     ~~^~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 662.1   221 |       __VA_ARGS__                                                           \
#11 662.1       |       ^~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 662.1    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:242:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 662.1   242 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)
#11 662.1       |   ^~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’
#11 662.1   246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))
#11 662.1       |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’
#11 662.1   124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), "mlp_backward", [&] {
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp:130:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘std::vector<at::Tensor>::size_type’ {aka ‘long unsigned int’} [-Wsign-compare]
#11 662.1   130 |     for (int i = 0; i < inputs.size(); i++) {
#11 662.1       |                     ~~^~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 662.1   221 |       __VA_ARGS__                                                           \
#11 662.1       |       ^~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 662.1    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:242:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 662.1   242 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)
#11 662.1       |   ^~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’
#11 662.1   246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))
#11 662.1       |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’
#11 662.1   124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), "mlp_backward", [&] {
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp:138:98: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]
#11 662.1   138 |     auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type());
#11 662.1       |                                                                                    ~~~~~~~~~~~~~~^~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 662.1   221 |       __VA_ARGS__                                                           \
#11 662.1       |       ^~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 662.1    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:242:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 662.1   242 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)
#11 662.1       |   ^~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’
#11 662.1   246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))
#11 662.1       |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’
#11 662.1   124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), "mlp_backward", [&] {
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 662.1                  from /app/apex/csrc/mlp.cpp:1:
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here
#11 662.1   225 |   DeprecatedTypeProperties & type() const {
#11 662.1       |                              ^~~~
#11 662.1 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,
#11 662.1                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 662.1                  from /app/apex/csrc/mlp.cpp:1:
#11 662.1 /app/apex/csrc/mlp.cpp:140:10: warning: unused variable ‘result’ [-Wunused-variable]
#11 662.1   140 |     auto result = mlp_bp<scalar_t>(
#11 662.1       |          ^~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 662.1   221 |       __VA_ARGS__                                                           \
#11 662.1       |       ^~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 662.1    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:242:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 662.1   242 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)
#11 662.1       |   ^~~~~~~~~~~~~~~~
#11 662.1 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’
#11 662.1   246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))
#11 662.1       |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 662.1 /app/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’
#11 662.1   124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), "mlp_backward", [&] {
#11 662.1       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 885.2 [2/2] /usr/local/cuda/bin/nvcc  -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c -c /app/apex/csrc/mlp_cuda.cu -o /app/apex/build/temp.linux-x86_64-3.10/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_62,code=sm_62 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_90,code=sm_90 -std=c++17
#11 885.2 x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 /app/apex/build/temp.linux-x86_64-3.10/csrc/mlp.o /app/apex/build/temp.linux-x86_64-3.10/csrc/mlp_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.10/mlp_cuda.cpython-310-x86_64-linux-gnu.so
#11 885.6 building 'fused_dense_cuda' extension
#11 885.6 Emitting ninja build file /app/apex/build/temp.linux-x86_64-3.10/build.ninja...
#11 885.6 Compiling objects...
#11 885.6 Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
#11 908.8 [1/2] c++ -MMD -MF /app/apex/build/temp.linux-x86_64-3.10/csrc/fused_dense.o.d -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c -c /app/apex/csrc/fused_dense.cpp -o /app/apex/build/temp.linux-x86_64-3.10/csrc/fused_dense.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=fused_dense_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
#11 908.8 /app/apex/csrc/fused_dense.cpp: In function ‘at::Tensor linear_bias_forward(at::Tensor, at::Tensor, at::Tensor)’:
#11 908.8 /app/apex/csrc/fused_dense.cpp:30:62: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]
#11 908.8    30 |   auto out = at::empty({batch_size, out_features}, input.type());
#11 908.8       |                                                    ~~~~~~~~~~^~
#11 908.8 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 908.8                  from /app/apex/csrc/fused_dense.cpp:1:
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here
#11 908.8   225 |   DeprecatedTypeProperties & type() const {
#11 908.8       |                              ^~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:33:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]
#11 908.8    33 |   auto lt_workspace = at::empty({1 << 22}, input.type());
#11 908.8       |                                            ~~~~~~~~~~^~
#11 908.8 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 908.8                  from /app/apex/csrc/fused_dense.cpp:1:
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here
#11 908.8   225 |   DeprecatedTypeProperties & type() const {
#11 908.8       |                              ^~~~
#11 908.8 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 908.8                  from /app/apex/csrc/fused_dense.cpp:1:
#11 908.8 /app/apex/csrc/fused_dense.cpp: In lambda function:
#11 908.8 /app/apex/csrc/fused_dense.cpp:37:15: warning: unused variable ‘b_ptr’ [-Wunused-variable]
#11 908.8    37 |     scalar_t* b_ptr = bias.data_ptr<scalar_t>();
#11 908.8       |               ^~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 908.8   221 |       __VA_ARGS__                                                           \
#11 908.8       |       ^~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 908.8    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:233:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 908.8   233 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \
#11 908.8       |   ^~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:267:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’
#11 908.8   267 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’
#11 908.8   276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \
#11 908.8       |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’
#11 908.8    35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), "linear_bias_forward", [&] {
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:38:10: warning: unused variable ‘result’ [-Wunused-variable]
#11 908.8    38 |     auto result = linear_bias_forward_cuda<scalar_t>(
#11 908.8       |          ^~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 908.8   221 |       __VA_ARGS__                                                           \
#11 908.8       |       ^~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 908.8    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:233:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 908.8   233 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \
#11 908.8       |   ^~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:267:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’
#11 908.8   267 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’
#11 908.8   276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \
#11 908.8       |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’
#11 908.8    35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), "linear_bias_forward", [&] {
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp: In lambda function:
#11 908.8 /app/apex/csrc/fused_dense.cpp:37:15: warning: unused variable ‘b_ptr’ [-Wunused-variable]
#11 908.8    37 |     scalar_t* b_ptr = bias.data_ptr<scalar_t>();
#11 908.8       |               ^~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 908.8   221 |       __VA_ARGS__                                                           \
#11 908.8       |       ^~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 908.8    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 908.8   234 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:267:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’
#11 908.8   267 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’
#11 908.8   276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \
#11 908.8       |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’
#11 908.8    35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), "linear_bias_forward", [&] {
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:38:10: warning: unused variable ‘result’ [-Wunused-variable]
#11 908.8    38 |     auto result = linear_bias_forward_cuda<scalar_t>(
#11 908.8       |          ^~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 908.8   221 |       __VA_ARGS__                                                           \
#11 908.8       |       ^~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 908.8    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 908.8   234 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:267:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’
#11 908.8   267 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’
#11 908.8   276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \
#11 908.8       |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’
#11 908.8    35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), "linear_bias_forward", [&] {
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp: In lambda function:
#11 908.8 /app/apex/csrc/fused_dense.cpp:37:15: warning: unused variable ‘b_ptr’ [-Wunused-variable]
#11 908.8    37 |     scalar_t* b_ptr = bias.data_ptr<scalar_t>();
#11 908.8       |               ^~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 908.8   221 |       __VA_ARGS__                                                           \
#11 908.8       |       ^~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 908.8    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:268:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 908.8   268 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \
#11 908.8       |   ^~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’
#11 908.8   276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \
#11 908.8       |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’
#11 908.8    35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), "linear_bias_forward", [&] {
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:38:10: warning: unused variable ‘result’ [-Wunused-variable]
#11 908.8    38 |     auto result = linear_bias_forward_cuda<scalar_t>(
#11 908.8       |          ^~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 908.8   221 |       __VA_ARGS__                                                           \
#11 908.8       |       ^~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 908.8    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:268:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 908.8   268 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \
#11 908.8       |   ^~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’
#11 908.8   276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \
#11 908.8       |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’
#11 908.8    35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), "linear_bias_forward", [&] {
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp: In lambda function:
#11 908.8 /app/apex/csrc/fused_dense.cpp:37:15: warning: unused variable ‘b_ptr’ [-Wunused-variable]
#11 908.8    37 |     scalar_t* b_ptr = bias.data_ptr<scalar_t>();
#11 908.8       |               ^~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 908.8   221 |       __VA_ARGS__                                                           \
#11 908.8       |       ^~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 908.8    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:269:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 908.8   269 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’
#11 908.8   276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \
#11 908.8       |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’
#11 908.8    35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), "linear_bias_forward", [&] {
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:38:10: warning: unused variable ‘result’ [-Wunused-variable]
#11 908.8    38 |     auto result = linear_bias_forward_cuda<scalar_t>(
#11 908.8       |          ^~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 908.8   221 |       __VA_ARGS__                                                           \
#11 908.8       |       ^~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 908.8    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:269:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 908.8   269 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’
#11 908.8   276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \
#11 908.8       |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’
#11 908.8    35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), "linear_bias_forward", [&] {
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp: In function ‘std::vector<at::Tensor> linear_bias_backward(at::Tensor, at::Tensor, at::Tensor)’:
#11 908.8 /app/apex/csrc/fused_dense.cpp:64:68: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]
#11 908.8    64 |   auto d_weight = at::empty({out_features, in_features}, input.type());
#11 908.8       |                                                          ~~~~~~~~~~^~
#11 908.8 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 908.8                  from /app/apex/csrc/fused_dense.cpp:1:
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here
#11 908.8   225 |   DeprecatedTypeProperties & type() const {
#11 908.8       |                              ^~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:68:53: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]
#11 908.8    68 |   auto d_bias = at::empty({out_features}, input.type());
#11 908.8       |                                           ~~~~~~~~~~^~
#11 908.8 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 908.8                  from /app/apex/csrc/fused_dense.cpp:1:
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here
#11 908.8   225 |   DeprecatedTypeProperties & type() const {
#11 908.8       |                              ^~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:70:65: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]
#11 908.8    70 |   auto d_input = at::empty({batch_size, in_features}, input.type());
#11 908.8       |                                                       ~~~~~~~~~~^~
#11 908.8 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 908.8                  from /app/apex/csrc/fused_dense.cpp:1:
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here
#11 908.8   225 |   DeprecatedTypeProperties & type() const {
#11 908.8       |                              ^~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:73:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]
#11 908.8    73 |   auto lt_workspace = at::empty({1 << 22}, input.type());
#11 908.8       |                                            ~~~~~~~~~~^~
#11 908.8 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 908.8                  from /app/apex/csrc/fused_dense.cpp:1:
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here
#11 908.8   225 |   DeprecatedTypeProperties & type() const {
#11 908.8       |                              ^~~~
#11 908.8 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 908.8                  from /app/apex/csrc/fused_dense.cpp:1:
#11 908.8 /app/apex/csrc/fused_dense.cpp: In lambda function:
#11 908.8 /app/apex/csrc/fused_dense.cpp:77:15: warning: unused variable ‘d_b_ptr’ [-Wunused-variable]
#11 908.8    77 |     scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();
#11 908.8       |               ^~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 908.8   221 |       __VA_ARGS__                                                           \
#11 908.8       |       ^~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 908.8    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:233:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 908.8   233 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \
#11 908.8       |   ^~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:267:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’
#11 908.8   267 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’
#11 908.8   276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \
#11 908.8       |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’
#11 908.8    75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), "linear_bias_backward", [&] {
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]
#11 908.8    78 |     auto result = linear_bias_backward_cuda<scalar_t>(
#11 908.8       |          ^~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 908.8   221 |       __VA_ARGS__                                                           \
#11 908.8       |       ^~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 908.8    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:233:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 908.8   233 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \
#11 908.8       |   ^~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:267:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’
#11 908.8   267 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’
#11 908.8   276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \
#11 908.8       |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’
#11 908.8    75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), "linear_bias_backward", [&] {
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp: In lambda function:
#11 908.8 /app/apex/csrc/fused_dense.cpp:77:15: warning: unused variable ‘d_b_ptr’ [-Wunused-variable]
#11 908.8    77 |     scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();
#11 908.8       |               ^~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 908.8   221 |       __VA_ARGS__                                                           \
#11 908.8       |       ^~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 908.8    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 908.8   234 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:267:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’
#11 908.8   267 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’
#11 908.8   276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \
#11 908.8       |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’
#11 908.8    75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), "linear_bias_backward", [&] {
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]
#11 908.8    78 |     auto result = linear_bias_backward_cuda<scalar_t>(
#11 908.8       |          ^~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 908.8   221 |       __VA_ARGS__                                                           \
#11 908.8       |       ^~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 908.8    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 908.8   234 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:267:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’
#11 908.8   267 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’
#11 908.8   276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \
#11 908.8       |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’
#11 908.8    75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), "linear_bias_backward", [&] {
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp: In lambda function:
#11 908.8 /app/apex/csrc/fused_dense.cpp:77:15: warning: unused variable ‘d_b_ptr’ [-Wunused-variable]
#11 908.8    77 |     scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();
#11 908.8       |               ^~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 908.8   221 |       __VA_ARGS__                                                           \
#11 908.8       |       ^~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 908.8    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:268:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 908.8   268 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \
#11 908.8       |   ^~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’
#11 908.8   276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \
#11 908.8       |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’
#11 908.8    75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), "linear_bias_backward", [&] {
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]
#11 908.8    78 |     auto result = linear_bias_backward_cuda<scalar_t>(
#11 908.8       |          ^~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 908.8   221 |       __VA_ARGS__                                                           \
#11 908.8       |       ^~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 908.8    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:268:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 908.8   268 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \
#11 908.8       |   ^~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’
#11 908.8   276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \
#11 908.8       |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’
#11 908.8    75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), "linear_bias_backward", [&] {
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp: In lambda function:
#11 908.8 /app/apex/csrc/fused_dense.cpp:77:15: warning: unused variable ‘d_b_ptr’ [-Wunused-variable]
#11 908.8    77 |     scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();
#11 908.8       |               ^~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 908.8   221 |       __VA_ARGS__                                                           \
#11 908.8       |       ^~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 908.8    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:269:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 908.8   269 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’
#11 908.8   276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \
#11 908.8       |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’
#11 908.8    75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), "linear_bias_backward", [&] {
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]
#11 908.8    78 |     auto result = linear_bias_backward_cuda<scalar_t>(
#11 908.8       |          ^~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 908.8   221 |       __VA_ARGS__                                                           \
#11 908.8       |       ^~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 908.8    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:269:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 908.8   269 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’
#11 908.8   276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \
#11 908.8       |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’
#11 908.8    75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), "linear_bias_backward", [&] {
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp: In function ‘std::vector<at::Tensor> linear_gelu_linear_forward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor)’:
#11 908.8 /app/apex/csrc/fused_dense.cpp:106:69: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]
#11 908.8   106 |   auto output1 = at::empty({batch_size, hidden_features}, input.type());
#11 908.8       |                                                           ~~~~~~~~~~^~
#11 908.8 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 908.8                  from /app/apex/csrc/fused_dense.cpp:1:
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here
#11 908.8   225 |   DeprecatedTypeProperties & type() const {
#11 908.8       |                              ^~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:107:69: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]
#11 908.8   107 |   auto gelu_in = at::empty({batch_size, hidden_features}, input.type());
#11 908.8       |                                                           ~~~~~~~~~~^~
#11 908.8 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 908.8                  from /app/apex/csrc/fused_dense.cpp:1:
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here
#11 908.8   225 |   DeprecatedTypeProperties & type() const {
#11 908.8       |                              ^~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:108:66: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]
#11 908.8   108 |   auto output2 = at::empty({batch_size, out_features}, input.type());
#11 908.8       |                                                        ~~~~~~~~~~^~
#11 908.8 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 908.8                  from /app/apex/csrc/fused_dense.cpp:1:
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here
#11 908.8   225 |   DeprecatedTypeProperties & type() const {
#11 908.8       |                              ^~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:111:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]
#11 908.8   111 |   auto lt_workspace = at::empty({1 << 22}, input.type());
#11 908.8       |                                            ~~~~~~~~~~^~
#11 908.8 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 908.8                  from /app/apex/csrc/fused_dense.cpp:1:
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here
#11 908.8   225 |   DeprecatedTypeProperties & type() const {
#11 908.8       |                              ^~~~
#11 908.8 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 908.8                  from /app/apex/csrc/fused_dense.cpp:1:
#11 908.8 /app/apex/csrc/fused_dense.cpp: In lambda function:
#11 908.8 /app/apex/csrc/fused_dense.cpp:118:10: warning: unused variable ‘result’ [-Wunused-variable]
#11 908.8   118 |     auto result = linear_gelu_linear_forward_cuda<scalar_t>(
#11 908.8       |          ^~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 908.8   221 |       __VA_ARGS__                                                           \
#11 908.8       |       ^~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 908.8    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:233:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 908.8   233 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \
#11 908.8       |   ^~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:267:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’
#11 908.8   267 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’
#11 908.8   276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \
#11 908.8       |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’
#11 908.8   113 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), "linear_gelu_linear_forward", [&] {
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp: In lambda function:
#11 908.8 /app/apex/csrc/fused_dense.cpp:118:10: warning: unused variable ‘result’ [-Wunused-variable]
#11 908.8   118 |     auto result = linear_gelu_linear_forward_cuda<scalar_t>(
#11 908.8       |          ^~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 908.8   221 |       __VA_ARGS__                                                           \
#11 908.8       |       ^~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 908.8    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 908.8   234 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:267:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’
#11 908.8   267 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’
#11 908.8   276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \
#11 908.8       |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’
#11 908.8   113 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), "linear_gelu_linear_forward", [&] {
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp: In lambda function:
#11 908.8 /app/apex/csrc/fused_dense.cpp:118:10: warning: unused variable ‘result’ [-Wunused-variable]
#11 908.8   118 |     auto result = linear_gelu_linear_forward_cuda<scalar_t>(
#11 908.8       |          ^~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 908.8   221 |       __VA_ARGS__                                                           \
#11 908.8       |       ^~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 908.8    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:268:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 908.8   268 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \
#11 908.8       |   ^~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’
#11 908.8   276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \
#11 908.8       |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’
#11 908.8   113 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), "linear_gelu_linear_forward", [&] {
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp: In lambda function:
#11 908.8 /app/apex/csrc/fused_dense.cpp:118:10: warning: unused variable ‘result’ [-Wunused-variable]
#11 908.8   118 |     auto result = linear_gelu_linear_forward_cuda<scalar_t>(
#11 908.8       |          ^~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 908.8   221 |       __VA_ARGS__                                                           \
#11 908.8       |       ^~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 908.8    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:269:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 908.8   269 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’
#11 908.8   276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \
#11 908.8       |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’
#11 908.8   113 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), "linear_gelu_linear_forward", [&] {
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp: In function ‘std::vector<at::Tensor> linear_gelu_linear_backward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor)’:
#11 908.8 /app/apex/csrc/fused_dense.cpp:149:72: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]
#11 908.8   149 |   auto d_weight1 = at::empty({hidden_features, in_features}, input.type());
#11 908.8       |                                                              ~~~~~~~~~~^~
#11 908.8 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 908.8                  from /app/apex/csrc/fused_dense.cpp:1:
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here
#11 908.8   225 |   DeprecatedTypeProperties & type() const {
#11 908.8       |                              ^~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:150:73: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]
#11 908.8   150 |   auto d_weight2 = at::empty({out_features, hidden_features}, input.type());
#11 908.8       |                                                               ~~~~~~~~~~^~
#11 908.8 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 908.8                  from /app/apex/csrc/fused_dense.cpp:1:
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here
#11 908.8   225 |   DeprecatedTypeProperties & type() const {
#11 908.8       |                              ^~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:151:57: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]
#11 908.8   151 |   auto d_bias1 = at::empty({hidden_features}, input.type());
#11 908.8       |                                               ~~~~~~~~~~^~
#11 908.8 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 908.8                  from /app/apex/csrc/fused_dense.cpp:1:
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here
#11 908.8   225 |   DeprecatedTypeProperties & type() const {
#11 908.8       |                              ^~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:152:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]
#11 908.8   152 |   auto d_bias2 = at::empty({out_features}, input.type());
#11 908.8       |                                            ~~~~~~~~~~^~
#11 908.8 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 908.8                  from /app/apex/csrc/fused_dense.cpp:1:
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here
#11 908.8   225 |   DeprecatedTypeProperties & type() const {
#11 908.8       |                              ^~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:153:65: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]
#11 908.8   153 |   auto d_input = at::empty({batch_size, in_features}, input.type());
#11 908.8       |                                                       ~~~~~~~~~~^~
#11 908.8 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 908.8                  from /app/apex/csrc/fused_dense.cpp:1:
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here
#11 908.8   225 |   DeprecatedTypeProperties & type() const {
#11 908.8       |                              ^~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:154:71: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]
#11 908.8   154 |   auto d_output1 = at::empty({batch_size, hidden_features}, input.type());
#11 908.8       |                                                             ~~~~~~~~~~^~
#11 908.8 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 908.8                  from /app/apex/csrc/fused_dense.cpp:1:
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here
#11 908.8   225 |   DeprecatedTypeProperties & type() const {
#11 908.8       |                              ^~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:157:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]
#11 908.8   157 |   auto lt_workspace = at::empty({1 << 22}, input.type());
#11 908.8       |                                            ~~~~~~~~~~^~
#11 908.8 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 908.8                  from /app/apex/csrc/fused_dense.cpp:1:
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here
#11 908.8   225 |   DeprecatedTypeProperties & type() const {
#11 908.8       |                              ^~~~
#11 908.8 In file included from /usr/local/lib/python3.10/dist-packages/torch/include/ATen/ATen.h:11,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,
#11 908.8                  from /usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5,
#11 908.8                  from /app/apex/csrc/fused_dense.cpp:1:
#11 908.8 /app/apex/csrc/fused_dense.cpp: In lambda function:
#11 908.8 /app/apex/csrc/fused_dense.cpp:163:10: warning: unused variable ‘result’ [-Wunused-variable]
#11 908.8   163 |     auto result = linear_gelu_linear_backward_cuda<scalar_t>(
#11 908.8       |          ^~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 908.8   221 |       __VA_ARGS__                                                           \
#11 908.8       |       ^~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 908.8    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:233:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 908.8   233 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \
#11 908.8       |   ^~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:267:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’
#11 908.8   267 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’
#11 908.8   276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \
#11 908.8       |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’
#11 908.8   159 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), "linear_bias_backward", [&] {
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp: In lambda function:
#11 908.8 /app/apex/csrc/fused_dense.cpp:163:10: warning: unused variable ‘result’ [-Wunused-variable]
#11 908.8   163 |     auto result = linear_gelu_linear_backward_cuda<scalar_t>(
#11 908.8       |          ^~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 908.8   221 |       __VA_ARGS__                                                           \
#11 908.8       |       ^~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 908.8    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:234:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 908.8   234 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:267:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’
#11 908.8   267 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’
#11 908.8   276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \
#11 908.8       |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’
#11 908.8   159 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), "linear_bias_backward", [&] {
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp: In lambda function:
#11 908.8 /app/apex/csrc/fused_dense.cpp:163:10: warning: unused variable ‘result’ [-Wunused-variable]
#11 908.8   163 |     auto result = linear_gelu_linear_backward_cuda<scalar_t>(
#11 908.8       |          ^~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 908.8   221 |       __VA_ARGS__                                                           \
#11 908.8       |       ^~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 908.8    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:268:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 908.8   268 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \
#11 908.8       |   ^~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’
#11 908.8   276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \
#11 908.8       |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’
#11 908.8   159 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), "linear_bias_backward", [&] {
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp: In lambda function:
#11 908.8 /app/apex/csrc/fused_dense.cpp:163:10: warning: unused variable ‘result’ [-Wunused-variable]
#11 908.8   163 |     auto result = linear_gelu_linear_backward_cuda<scalar_t>(
#11 908.8       |          ^~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’
#11 908.8   221 |       __VA_ARGS__                                                           \
#11 908.8       |       ^~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’
#11 908.8    74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:269:3: note: in expansion of macro ‘AT_DISPATCH_CASE’
#11 908.8   269 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)
#11 908.8       |   ^~~~~~~~~~~~~~~~
#11 908.8 /usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’
#11 908.8   276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \
#11 908.8       |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 908.8 /app/apex/csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’
#11 908.8   159 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), "linear_bias_backward", [&] {
#11 908.8       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#11 1129.1 [2/2] /usr/local/cuda/bin/nvcc  -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c -c /app/apex/csrc/fused_dense_cuda.cu -o /app/apex/build/temp.linux-x86_64-3.10/csrc/fused_dense_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=fused_dense_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_62,code=sm_62 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_90,code=sm_90 -std=c++17
#11 1129.1 x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 /app/apex/build/temp.linux-x86_64-3.10/csrc/fused_dense.o /app/apex/build/temp.linux-x86_64-3.10/csrc/fused_dense_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.10/fused_dense_cuda.cpython-310-x86_64-linux-gnu.so
#11 1129.5 building 'scaled_upper_triang_masked_softmax_cuda' extension
#11 1129.5 creating /app/apex/build/temp.linux-x86_64-3.10/csrc/megatron
#11 1129.6 Emitting ninja build file /app/apex/build/temp.linux-x86_64-3.10/build.ninja...
#11 1129.6 Compiling objects...
#11 1129.6 Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
#11 1151.7 [1/2] c++ -MMD -MF /app/apex/build/temp.linux-x86_64-3.10/csrc/megatron/scaled_upper_triang_masked_softmax.o.d -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/app/apex/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c -c /app/apex/csrc/megatron/scaled_upper_triang_masked_softmax.cpp -o /app/apex/build/temp.linux-x86_64-3.10/csrc/megatron/scaled_upper_triang_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
